// lexer.uya - 词法分析器
// 从 compiler-mini/src/lexer.c 重新覆盖式翻译
//
// 注意：Uya 没有 union，本模块无 union
// 注意：Uya 不支持指针运算，ptr + i 转换为 &ptr[i]
// 注意：isalpha/isdigit 等用字符比较实现

// Token 类型枚举（与 C 版本 lexer.h 一致）
enum TokenType {
    TOKEN_EOF,
    TOKEN_IDENTIFIER,
    TOKEN_NUMBER,
    TOKEN_FLOAT,
    TOKEN_STRING,
    // 字符串插值相关（"text${expr}text" 或 "text${expr:spec}text"）
    TOKEN_INTERP_TEXT,
    TOKEN_INTERP_OPEN,
    TOKEN_INTERP_CLOSE,
    TOKEN_INTERP_SPEC,
    TOKEN_INTERP_END,
    TOKEN_ENUM,
    TOKEN_STRUCT,
    TOKEN_CONST,
    TOKEN_VAR,
    TOKEN_FN,
    TOKEN_EXTERN,
    TOKEN_RETURN,
    TOKEN_IF,
    TOKEN_ELSE,
    TOKEN_WHILE,
    TOKEN_FOR,
    TOKEN_BREAK,
    TOKEN_CONTINUE,
    TOKEN_TRUE,
    TOKEN_FALSE,
    TOKEN_NULL,
    TOKEN_AT_IDENTIFIER,  // @ 后跟内置函数标识符（@sizeof、@alignof、@len、@max、@min）
    TOKEN_AS,
    TOKEN_PLUS,
    TOKEN_MINUS,
    TOKEN_ASTERISK,
    TOKEN_SLASH,
    TOKEN_PERCENT,
    TOKEN_EQUAL,
    TOKEN_NOT_EQUAL,
    TOKEN_LESS,
    TOKEN_GREATER,
    TOKEN_LESS_EQUAL,
    TOKEN_GREATER_EQUAL,
    TOKEN_LOGICAL_AND,
    TOKEN_LOGICAL_OR,
    TOKEN_EXCLAMATION,
    TOKEN_AMPERSAND,
    TOKEN_PIPE,
    TOKEN_CARET,
    TOKEN_TILDE,
    TOKEN_LSHIFT,
    TOKEN_RSHIFT,
    TOKEN_LEFT_PAREN,
    TOKEN_RIGHT_PAREN,
    TOKEN_LEFT_BRACE,
    TOKEN_RIGHT_BRACE,
    TOKEN_LEFT_BRACKET,
    TOKEN_RIGHT_BRACKET,
    TOKEN_SEMICOLON,
    TOKEN_COMMA,
    TOKEN_ASSIGN,
    TOKEN_DOT,
    TOKEN_COLON,
    TOKEN_ELLIPSIS,
}

// Token 结构体
struct Token {
    type: TokenType,
    value: &byte,  // 可为 null，存储在 Arena 中
    line: i32,
    column: i32,
}

// 1MB 缓冲区大小（与 C 版本 LEXER_BUFFER_SIZE 一致）
const LEXER_BUFFER_SIZE: i32 = 1024 * 1024;

// 字符串插值内部缓冲区大小（read_string 也复用此缓冲，避免栈上再分配 4096）
const LEXER_STRING_INTERP_BUFFER_SIZE: i32 = 4096;

// Lexer 结构体
struct Lexer {
    buffer: [byte: LEXER_BUFFER_SIZE],
    buffer_size: usize,
    position: usize,
    line: i32,
    column: i32,
    filename: &byte,  // 可为 null，存储在 Arena 中
    // 字符串插值状态（仅当 string_mode 或 interp_depth 非 0 时有效）
    string_mode: i32,                 // 1 表示在 "..." 内
    interp_depth: i32,                // 在 ${ } 内时为 1
    pending_interp_open: i32,         // 1 表示已返回 INTERP_TEXT，下一 token 应为 INTERP_OPEN
    reading_spec: i32,                // 1 表示正在读取 :spec 直到 }
    has_seen_interp_in_string: i32,   // 当前字符串中是否出现过 ${
    string_text_buffer: [byte: LEXER_STRING_INTERP_BUFFER_SIZE],
    string_text_len: usize,
}

// 辅助：查看指定偏移的字符
fn peek_char(lexer: &Lexer, offset: usize) byte {
    const pos: usize = lexer.position + offset;
    if pos >= lexer.buffer_size {
        return 0 as byte;
    }
    return lexer.buffer[pos] as byte;
}

// 辅助：前进一个字符，更新位置信息
fn advance_char(lexer: &Lexer) byte {
    if lexer.position >= lexer.buffer_size {
        return 0 as byte;
    }
    const c: byte = lexer.buffer[lexer.position];
    lexer.position = lexer.position + 1;
    if c == 10 {
        lexer.line = lexer.line + 1;
        lexer.column = 1;
    } else {
        lexer.column = lexer.column + 1;
    }
    return c;
}

// 跳过空白字符和注释
fn skip_whitespace_and_comments(lexer: &Lexer) void {
    while lexer.position < lexer.buffer_size {
        const c: byte = peek_char(lexer, 0);
        if c == 32 || c == 9 || c == 13 || c == 10 {
            advance_char(lexer);
        } else if c == 47 && peek_char(lexer, 1) == 47 {
            while lexer.position < lexer.buffer_size && peek_char(lexer, 0) != 10 {
                advance_char(lexer);
            }
        } else {
            return;
        }
    }
}

// 字符串相等比较（用于关键字判断，Uya 无 strcmp）
fn str_equals_lexer(s1: &byte, s2: &byte) i32 {
    if s1 == null && s2 == null {
        return 1;
    }
    if s1 == null || s2 == null {
        return 0;
    }
    var i: i32 = 0;
    while true {
        const c1: byte = s1[i] as byte;
        const c2: byte = s2[i] as byte;
        if c1 != c2 {
            return 0;
        }
        if c1 == 0 {
            return 1;
        }
        i = i + 1;
    }
    return 0;
}

// 检查是否为关键字
fn is_keyword(str: &byte) TokenType {
    var result: TokenType = TokenType.TOKEN_IDENTIFIER;
    if str_equals_lexer(str, "enum" as &byte) != 0 {
        result = TokenType.TOKEN_ENUM;
    } else if str_equals_lexer(str, "struct" as &byte) != 0 {
        result = TokenType.TOKEN_STRUCT;
    } else if str_equals_lexer(str, "const" as &byte) != 0 {
        result = TokenType.TOKEN_CONST;
    } else if str_equals_lexer(str, "var" as &byte) != 0 {
        result = TokenType.TOKEN_VAR;
    } else if str_equals_lexer(str, "fn" as &byte) != 0 {
        result = TokenType.TOKEN_FN;
    } else if str_equals_lexer(str, "extern" as &byte) != 0 {
        result = TokenType.TOKEN_EXTERN;
    } else if str_equals_lexer(str, "return" as &byte) != 0 {
        result = TokenType.TOKEN_RETURN;
    } else if str_equals_lexer(str, "if" as &byte) != 0 {
        result = TokenType.TOKEN_IF;
    } else if str_equals_lexer(str, "else" as &byte) != 0 {
        result = TokenType.TOKEN_ELSE;
    } else if str_equals_lexer(str, "while" as &byte) != 0 {
        result = TokenType.TOKEN_WHILE;
    } else if str_equals_lexer(str, "for" as &byte) != 0 {
        result = TokenType.TOKEN_FOR;
    } else if str_equals_lexer(str, "break" as &byte) != 0 {
        result = TokenType.TOKEN_BREAK;
    } else if str_equals_lexer(str, "continue" as &byte) != 0 {
        result = TokenType.TOKEN_CONTINUE;
    } else if str_equals_lexer(str, "true" as &byte) != 0 {
        result = TokenType.TOKEN_TRUE;
    } else if str_equals_lexer(str, "false" as &byte) != 0 {
        result = TokenType.TOKEN_FALSE;
    } else if str_equals_lexer(str, "null" as &byte) != 0 {
        result = TokenType.TOKEN_NULL;
    } else if str_equals_lexer(str, "as" as &byte) != 0 {
        result = TokenType.TOKEN_AS;
    }
    return result;
}

// 从 Arena 复制指定长度的字符串（含结尾 null）
fn arena_copy_str(arena: &Arena, start: &byte, n: usize) &byte {
    if arena == null || start == null {
        return null;
    }
    const result: &byte = arena_alloc(arena, n + 1) as &byte;
    if result == null {
        return null;
    }
    var i: usize = 0;
    while i < n {
        result[i] = start[i];
        i = i + 1;
    }
    result[n] = 0;
    return result;
}

// 创建 Token
fn make_token(arena: &Arena, type: TokenType, value: &byte, line: i32, column: i32) &Token {
    if arena == null {
        return null;
    }
    const token: &Token = arena_alloc(arena, @sizeof(Token)) as &Token;
    if token == null {
        return null;
    }
    token.type = type;
    token.value = value;
    token.line = line;
    token.column = column;
    return token;
}

// 读取标识符或关键字
fn read_identifier_or_keyword(lexer: &Lexer, arena: &Arena) &Token {
    const start_index: usize = lexer.position;
    const line: i32 = lexer.line;
    const column: i32 = lexer.column;
    while lexer.position < lexer.buffer_size {
        const c: byte = peek_char(lexer, 0);
        const cu: i32 = c as i32;
        var is_alpha: i32 = 0;
        if (cu >= 97 && cu <= 122) || (cu >= 65 && cu <= 90) {
            is_alpha = 1;
        }
        var is_digit: i32 = 0;
        if cu >= 48 && cu <= 57 {
            is_digit = 1;
        }
        if is_alpha != 0 || is_digit != 0 || c == 95 {
            advance_char(lexer);
        } else {
            break;
        }
    }
    const n: usize = lexer.position - start_index;
    if n <= 0 || n > 256 {
        return null;
    }
    const start_ptr: &byte = &lexer.buffer[start_index] as &byte;
    const value: &byte = arena_copy_str(arena, start_ptr, n);
    if value == null {
        return null;
    }
    const tok_type: TokenType = is_keyword(value);
    return make_token(arena, tok_type, value, line, column);
}

// 读取数字字面量（整数或浮点数）
fn read_number(lexer: &Lexer, arena: &Arena) &Token {
    const start_index: usize = lexer.position;
    const line: i32 = lexer.line;
    const column: i32 = lexer.column;
    var is_float: i32 = 0;
    while lexer.position < lexer.buffer_size {
        const c: byte = peek_char(lexer, 0);
        if c >= 48 && c <= 57 {
            advance_char(lexer);
        } else {
            break;
        }
    }
    if peek_char(lexer, 0) == 46 {
        advance_char(lexer);
        is_float = 1;
        while lexer.position < lexer.buffer_size {
            const c: byte = peek_char(lexer, 0);
            if c >= 48 && c <= 57 {
                advance_char(lexer);
            } else {
                break;
            }
        }
    }
    const ec: byte = peek_char(lexer, 0);
    if ec == 101 || ec == 69 {
        advance_char(lexer);
        is_float = 1;
        if peek_char(lexer, 0) == 43 || peek_char(lexer, 0) == 45 {
            advance_char(lexer);
        }
        if peek_char(lexer, 0) < 48 || peek_char(lexer, 0) > 57 {
            return null;
        }
        while lexer.position < lexer.buffer_size {
            const c: byte = peek_char(lexer, 0);
            if c >= 48 && c <= 57 {
                advance_char(lexer);
            } else {
                break;
            }
        }
    }
    const n: usize = lexer.position - start_index;
    const start_ptr: &byte = &lexer.buffer[start_index] as &byte;
    const value: &byte = arena_copy_str(arena, start_ptr, n);
    if value == null {
        return null;
    }
    if is_float != 0 {
        return make_token(arena, TokenType.TOKEN_FLOAT, value, line, column);
    }
    return make_token(arena, TokenType.TOKEN_NUMBER, value, line, column);
}

// 读取字符串字面量（支持 \n \t \\ \" \0）
fn read_string(lexer: &Lexer, arena: &Arena) &Token {
    const line: i32 = lexer.line;
    const column: i32 = lexer.column;
    advance_char(lexer);
    lexer.string_text_len = 0;
    while lexer.position < lexer.buffer_size {
        const c: byte = peek_char(lexer, 0);
        if c == 0 {
            return null;
        }
        if c == 34 {
            break;
        }
        if c == 10 {
            return null;
        }
        if c == 92 {
            advance_char(lexer);
            if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
                return null;
            }
            const escape_char: byte = peek_char(lexer, 0);
            var escaped_value: byte = 0;
            if escape_char == 110 {
                escaped_value = 10;
            } else if escape_char == 116 {
                escaped_value = 9;
            } else if escape_char == 92 {
                escaped_value = 92;
            } else if escape_char == 34 {
                escaped_value = 34;
            } else if escape_char == 48 {
                escaped_value = 0;
            } else {
                return null;
            }
            lexer.string_text_buffer[lexer.string_text_len] = escaped_value;
            lexer.string_text_len = lexer.string_text_len + 1;
            advance_char(lexer);
        } else {
            if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
                return null;
            }
            lexer.string_text_buffer[lexer.string_text_len] = c;
            lexer.string_text_len = lexer.string_text_len + 1;
            advance_char(lexer);
        }
    }
    if lexer.position >= lexer.buffer_size || peek_char(lexer, 0) != 34 {
        return null;
    }
    advance_char(lexer);
    const value: &byte = arena_alloc(arena, lexer.string_text_len + 1) as &byte;
    if value == null {
        return null;
    }
    var i: usize = 0;
    while i < lexer.string_text_len {
        value[i] = lexer.string_text_buffer[i];
        i = i + 1;
    }
    value[lexer.string_text_len] = 0;
    return make_token(arena, TokenType.TOKEN_STRING, value, line, column);
}

// 将字符串内一个字符（或转义序列）读入 string_text_buffer，成功返回 0，失败返回 -1
fn read_string_char_into_buffer(lexer: &Lexer) i32 {
    if lexer.position >= lexer.buffer_size {
        return -1;
    }
    const c: byte = peek_char(lexer, 0);
    if c == 92 {
        advance_char(lexer);
        if lexer.position >= lexer.buffer_size {
            return -1;
        }
        const e: byte = peek_char(lexer, 0);
        var escaped: byte = 0;
        if e == 110 {
            escaped = 10;
        } else if e == 116 {
            escaped = 9;
        } else if e == 92 {
            escaped = 92;
        } else if e == 34 {
            escaped = 34;
        } else if e == 48 {
            escaped = 0;
        } else {
            return -1;
        }
        if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
            return -1;
        }
        lexer.string_text_buffer[lexer.string_text_len] = escaped;
        lexer.string_text_len = lexer.string_text_len + 1;
        advance_char(lexer);
        return 0;
    }
    if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
        return -1;
    }
    lexer.string_text_buffer[lexer.string_text_len] = c;
    lexer.string_text_len = lexer.string_text_len + 1;
    advance_char(lexer);
    return 0;
}

// 获取下一个 Token
fn lexer_next_token(lexer: &Lexer, arena: &Arena) &Token {
    if lexer == null || arena == null {
        return null;
    }
    while true {
        // ----- 字符串插值 / 插值内部状态：优先于空白与普通 switch -----
        if lexer.reading_spec != 0 {
            lexer.string_text_len = 0;
            while lexer.position < lexer.buffer_size && peek_char(lexer, 0) != 125 {
                if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
                    return null;
                }
                lexer.string_text_buffer[lexer.string_text_len] = peek_char(lexer, 0);
                lexer.string_text_len = lexer.string_text_len + 1;
                advance_char(lexer);
            }
            if lexer.position >= lexer.buffer_size || peek_char(lexer, 0) != 125 {
                return null;
            }
            lexer.string_text_buffer[lexer.string_text_len] = 0;
            const spec_value: &byte = arena_copy_str(arena, &lexer.string_text_buffer[0] as &byte, lexer.string_text_len);
            if spec_value == null {
                return null;
            }
            advance_char(lexer);
            lexer.interp_depth = 0;
            lexer.reading_spec = 0;
            lexer.pending_interp_open = 0;
            return make_token(arena, TokenType.TOKEN_INTERP_SPEC, spec_value, lexer.line, lexer.column);
        }
        if lexer.pending_interp_open != 0 {
            if lexer.position + 2 > lexer.buffer_size ||
                peek_char(lexer, 0) != 36 || peek_char(lexer, 1) != 123 {
                return null;
            }
            advance_char(lexer);
            advance_char(lexer);
            lexer.pending_interp_open = 0;
            lexer.interp_depth = 1;
            return make_token(arena, TokenType.TOKEN_INTERP_OPEN, null, lexer.line, lexer.column);
        }
        if lexer.interp_depth > 0 {
            const p: byte = peek_char(lexer, 0);
            if p == 125 {
                advance_char(lexer);
                lexer.interp_depth = 0;
                return make_token(arena, TokenType.TOKEN_INTERP_CLOSE, null, lexer.line, lexer.column);
            }
            if p == 58 {
                advance_char(lexer);
                lexer.reading_spec = 1;
                lexer.string_text_len = 0;
                while lexer.position < lexer.buffer_size && peek_char(lexer, 0) != 125 {
                    if lexer.string_text_len >= (LEXER_STRING_INTERP_BUFFER_SIZE - 1) as usize {
                        return null;
                    }
                    lexer.string_text_buffer[lexer.string_text_len] = peek_char(lexer, 0);
                    lexer.string_text_len = lexer.string_text_len + 1;
                    advance_char(lexer);
                }
                if lexer.position >= lexer.buffer_size || peek_char(lexer, 0) != 125 {
                    return null;
                }
                lexer.string_text_buffer[lexer.string_text_len] = 0;
                const spec_value2: &byte = arena_copy_str(arena, &lexer.string_text_buffer[0] as &byte, lexer.string_text_len);
                if spec_value2 == null {
                    return null;
                }
                advance_char(lexer);
                lexer.interp_depth = 0;
                lexer.pending_interp_open = 0;
                lexer.reading_spec = 0;
                return make_token(arena, TokenType.TOKEN_INTERP_SPEC, spec_value2, lexer.line, lexer.column);
            }
        }
        if lexer.string_mode != 0 && lexer.interp_depth == 0 {
            lexer.string_text_len = 0;
            while lexer.position < lexer.buffer_size {
                const p: byte = peek_char(lexer, 0);
                if p == 34 {
                    if lexer.has_seen_interp_in_string == 0 {
                        lexer.string_text_buffer[lexer.string_text_len] = 0;
                        const value: &byte = arena_copy_str(arena, &lexer.string_text_buffer[0] as &byte, lexer.string_text_len);
                        if value == null {
                            return null;
                        }
                        advance_char(lexer);
                        lexer.string_mode = 0;
                        return make_token(arena, TokenType.TOKEN_STRING, value, lexer.line, lexer.column);
                    }
                    if lexer.string_text_len > 0 {
                        lexer.string_text_buffer[lexer.string_text_len] = 0;
                        const value2: &byte = arena_copy_str(arena, &lexer.string_text_buffer[0] as &byte, lexer.string_text_len);
                        if value2 == null {
                            return null;
                        }
                        lexer.string_text_len = 0;
                        return make_token(arena, TokenType.TOKEN_INTERP_TEXT, value2, lexer.line, lexer.column);
                    }
                    advance_char(lexer);
                    lexer.string_mode = 0;
                    lexer.has_seen_interp_in_string = 0;
                    return make_token(arena, TokenType.TOKEN_INTERP_END, null, lexer.line, lexer.column);
                }
                if p == 36 && peek_char(lexer, 1) == 123 {
                    lexer.has_seen_interp_in_string = 1;
                    lexer.pending_interp_open = 1;
                    lexer.string_text_buffer[lexer.string_text_len] = 0;
                    const value3: &byte = arena_copy_str(arena, &lexer.string_text_buffer[0] as &byte, lexer.string_text_len);
                    if value3 == null {
                        return null;
                    }
                    lexer.string_text_len = 0;
                    return make_token(arena, TokenType.TOKEN_INTERP_TEXT, value3, lexer.line, lexer.column);
                }
                if read_string_char_into_buffer(lexer) != 0 {
                    return null;
                }
            }
            return null;
        }
        skip_whitespace_and_comments(lexer);
        if lexer.position >= lexer.buffer_size {
            return make_token(arena, TokenType.TOKEN_EOF, null, lexer.line, lexer.column);
        }
        const c: byte = peek_char(lexer, 0);
        const line: i32 = lexer.line;
        const column: i32 = lexer.column;
        if c == 34 {
            advance_char(lexer);
            lexer.string_mode = 1;
            lexer.has_seen_interp_in_string = 0;
            lexer.string_text_len = 0;
            continue;
        }
        if c == 43 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_PLUS, "+" as &byte, line, column);
    }
    if c == 45 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_MINUS, "-" as &byte, line, column);
    }
    if c == 42 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_ASTERISK, "*" as &byte, line, column);
    }
    if c == 47 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_SLASH, "/" as &byte, line, column);
    }
    if c == 37 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_PERCENT, "%" as &byte, line, column);
    }
    if c == 61 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 61 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_EQUAL, "==" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_ASSIGN, "=" as &byte, line, column);
    }
    if c == 33 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 61 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_NOT_EQUAL, "!=" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_EXCLAMATION, "!" as &byte, line, column);
    }
    if c == 60 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 60 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_LSHIFT, "<<" as &byte, line, column);
        }
        if peek_char(lexer, 0) == 61 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_LESS_EQUAL, "<=" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_LESS, "<" as &byte, line, column);
    }
    if c == 62 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 62 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_RSHIFT, ">>" as &byte, line, column);
        }
        if peek_char(lexer, 0) == 61 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_GREATER_EQUAL, ">=" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_GREATER, ">" as &byte, line, column);
    }
    if c == 94 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_CARET, "^" as &byte, line, column);
    }
    if c == 126 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_TILDE, "~" as &byte, line, column);
    }
    if c == 38 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 38 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_LOGICAL_AND, "&&" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_AMPERSAND, "&" as &byte, line, column);
    }
    if c == 124 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 124 {
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_LOGICAL_OR, "||" as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_PIPE, "|" as &byte, line, column);
    }
    if c == 40 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_LEFT_PAREN, "(" as &byte, line, column);
    }
    if c == 41 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_RIGHT_PAREN, ")" as &byte, line, column);
    }
    if c == 123 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_LEFT_BRACE, "{" as &byte, line, column);
    }
    if c == 125 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_RIGHT_BRACE, "}" as &byte, line, column);
    }
    if c == 91 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_LEFT_BRACKET, "[" as &byte, line, column);
    }
    if c == 93 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_RIGHT_BRACKET, "]" as &byte, line, column);
    }
    if c == 59 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_SEMICOLON, ";" as &byte, line, column);
    }
    if c == 44 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_COMMA, "," as &byte, line, column);
    }
    if c == 46 {
        advance_char(lexer);
        if peek_char(lexer, 0) == 46 && peek_char(lexer, 1) == 46 {
            advance_char(lexer);
            advance_char(lexer);
            return make_token(arena, TokenType.TOKEN_ELLIPSIS, "..." as &byte, line, column);
        }
        return make_token(arena, TokenType.TOKEN_DOT, "." as &byte, line, column);
    }
    if c == 58 {
        advance_char(lexer);
        return make_token(arena, TokenType.TOKEN_COLON, ":" as &byte, line, column);
    }
    if c == 64 {
        advance_char(lexer);
        const next_c: byte = peek_char(lexer, 0);
        const next_cu: i32 = next_c as i32;
        var next_is_alpha: i32 = 0;
        if (next_cu >= 97 && next_cu <= 122) || (next_cu >= 65 && next_cu <= 90) {
            next_is_alpha = 1;
        }
        if next_is_alpha != 0 || next_c == 95 {
            const start_index: usize = lexer.position;
            while lexer.position < lexer.buffer_size {
                const cc: byte = peek_char(lexer, 0);
                const ccu: i32 = cc as i32;
                var cc_alpha: i32 = 0;
                if (ccu >= 97 && ccu <= 122) || (ccu >= 65 && ccu <= 90) {
                    cc_alpha = 1;
                }
                var cc_digit: i32 = 0;
                if ccu >= 48 && ccu <= 57 {
                    cc_digit = 1;
                }
                if cc_alpha != 0 || cc_digit != 0 || cc == 95 {
                    advance_char(lexer);
                } else {
                    break;
                }
            }
            const n: usize = lexer.position - start_index;
            if n <= 0 || n > 256 {
                return null;
            }
            const start_ptr: &byte = &lexer.buffer[start_index] as &byte;
            const value: &byte = arena_copy_str(arena, start_ptr, n);
            if value == null {
                return null;
            }
            if str_equals_lexer(value, "sizeof" as &byte) != 0 || str_equals_lexer(value, "alignof" as &byte) != 0 ||
                str_equals_lexer(value, "len" as &byte) != 0 || str_equals_lexer(value, "max" as &byte) != 0 ||
                str_equals_lexer(value, "min" as &byte) != 0 || str_equals_lexer(value, "params" as &byte) != 0 {
                return make_token(arena, TokenType.TOKEN_AT_IDENTIFIER, value, line, column);
            }
            return null;
        }
        return null;
    }
    const cu: i32 = c as i32;
    var is_alpha: i32 = 0;
    if (cu >= 97 && cu <= 122) || (cu >= 65 && cu <= 90) {
        is_alpha = 1;
    }
    var is_digit: i32 = 0;
    if cu >= 48 && cu <= 57 {
        is_digit = 1;
    }
    if is_alpha != 0 || c == 95 {
        return read_identifier_or_keyword(lexer, arena);
    }
    if is_digit != 0 {
        return read_number(lexer, arena);
    }
    advance_char(lexer);
    return make_token(arena, TokenType.TOKEN_EOF, null, line, column);
    }
}

// 初始化 Lexer
// 参数：lexer - 由调用者提供，source - 源代码，source_len - 长度，filename - 文件名（存 Arena），arena - Arena
// 返回：成功 0，失败 -1
fn lexer_init(lexer: &Lexer, source: &byte, source_len: usize, filename: &byte, arena: &Arena) i32 {
    if lexer == null || source == null || arena == null {
        return -1;
    }
    if source_len >= LEXER_BUFFER_SIZE as usize {
        return -1;
    }
    var i: usize = 0;
    while i < source_len {
        lexer.buffer[i] = source[i];
        i = i + 1;
    }
    lexer.buffer[source_len] = 0;
    lexer.buffer_size = source_len;
    lexer.position = 0;
    lexer.line = 1;
    lexer.column = 1;
    lexer.string_mode = 0;
    lexer.interp_depth = 0;
    lexer.pending_interp_open = 0;
    lexer.reading_spec = 0;
    lexer.string_text_len = 0;
    lexer.has_seen_interp_in_string = 0;
    if filename != null {
        var n: usize = 0;
        while filename[n] != 0 {
            n = n + 1;
        }
        n = n + 1;
        const filename_copy: &byte = arena_alloc(arena, n) as &byte;
        if filename_copy == null {
            return -1;
        }
        var j: usize = 0;
        while j < n {
            filename_copy[j] = filename[j];
            j = j + 1;
        }
        lexer.filename = filename_copy;
    } else {
        lexer.filename = null;
    }
    return 0;
}
